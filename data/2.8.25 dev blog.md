commit: bdfdd123d3e3d7b51129200c1a0430b9edaf79b9

Began working on the main chat loop of the RAG application. I wanted to use langChain but decided to try to implement everything manually to get a better understanding of each steps in a RAG application.

So far the loop decision looks like this.

1. ZeroShot classifier classifies user query as one of the following
    1. general_chat - directly prompt llm for response (no vector search)
    2. product_search - prompt vector search on the product_title milvus collection and generate a context-aware response
    3. review_search - same as vector search but using the review collection
  
Added a chat endpoint for ecommerce rag. This endpoint handles the above mentioned workflow. It also streams the responses token by token.


Next Steps:
1. implement review workflow in rag chat endpoint. Just follow product workflow. 
2. start to think about front end (stream lit?)
3. prompt tuning - the current prompt is not good (for vector search response)