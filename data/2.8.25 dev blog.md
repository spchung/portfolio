commit: 84339d023f3f23e0030c84cbd209e9683314fd7d

Began working on the main chat loop of the RAG application. I wanted to use langChain but decided to try to implement everything manually to get a better understanding of each steps in a RAG application.

So far the loop decision looks like this.

1. ZeroShot classifier classifies user query as one of the following
    1. general_chat - directly prompt llm for response (no vector search)
    2. product_search - prompt vector search on the product_title milvus collection and generate a context-aware response
    3. review_search - same as vector search but using the review collection


Next Steps:
1. start to think about front end (stream lit?)
2. prompt tuning - the current prompt is not good (for vector search response)